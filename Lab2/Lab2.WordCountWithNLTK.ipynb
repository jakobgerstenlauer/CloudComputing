{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.1: Word Count 1\n",
    "Here I read in a text file, tokenize it, and count all tokens. \n",
    "Then, the *n* most common tokens are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program will count all words and show you only the n most common words.\n",
      "Which ascii file should be processed?  FirstContactWithTensorFlow.txt\n",
      "Up to which rank n should the most common words be displayed?  10\n",
      "[('the', 1343), (',', 1251), ('.', 810), (')', 638), ('(', 637), ('of', 586), ('to', 491), ('a', 468), (':', 454), ('in', 417)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import codecs\n",
    "import os.path\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens(fileName):\n",
    "    #Check first if the file exists\n",
    "    if(os.path.isfile(fileName)):\n",
    "        tf = codecs.open(fileName, mode='r')\n",
    "        text = tf.read()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return tokens\n",
    "\n",
    "print (\"This program will count all words and show you only the n most common words.\")\n",
    "fileName = raw_input(\"Which ascii file should be processed?  \")\n",
    "\n",
    "if(os.path.isfile(fileName)):\n",
    "    n = int(raw_input(\"Up to which rank n should the most common words be displayed?  \"))\n",
    "    tokens = get_tokens(fileName)\n",
    "    count = Counter(tokens)\n",
    "    print count.most_common(n)\n",
    "else:\n",
    "    print(\"Error: This file does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem here, because special characters such as ', and '. are also considered.\n",
    "In order to improve the script I will now remove the punctuation marks. But first I count the total number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.2: Word Count 2\n",
    "I add two new lines in order to count and print the total number of words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program will count all words and show you only the n most common words.\n",
      "Which ascii file should be processed?  FirstContactWithTensorFlow.txt\n",
      "Up to which rank n should the most common words be displayed?  10\n",
      "[('the', 1343), (',', 1251), ('.', 810), (')', 638), ('(', 637), ('of', 586), ('to', 491), ('a', 468), (':', 454), ('in', 417)]\n",
      "The document contains 25155 words in total.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import codecs\n",
    "import os.path\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens(fileName):\n",
    "    #Check first if the file exists\n",
    "    if(os.path.isfile(fileName)):\n",
    "        tf = codecs.open(fileName, mode='r')\n",
    "        text = tf.read()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return tokens\n",
    "\n",
    "print (\"This program will count all words and show you only the n most common words.\")\n",
    "fileName = raw_input(\"Which ascii file should be processed?  \")\n",
    "\n",
    "if(os.path.isfile(fileName)):\n",
    "    n = int(raw_input(\"Up to which rank n should the most common words be displayed?  \"))\n",
    "    tokens = get_tokens(fileName)\n",
    "    count = Counter(tokens)\n",
    "    print count.most_common(n)\n",
    "    #count all the words in the document\n",
    "    numWords = sum(count.values()) \n",
    "    print (\"The document contains \"+ str(numWords) +\" words in total.\")\n",
    "else:\n",
    "    print(\"Error: This file does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.3: Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program will count all words and show you only the n most common words.\n",
      "Which ascii file should be processed?  FirstContactWithTensorFlow.txt\n",
      "Up to which rank n should the most common words be displayed?  10\n",
      "[('the', 1444), ('of', 586), ('to', 531), ('in', 506), ('a', 481), ('and', 346), ('is', 289), ('we', 279), ('that', 275), ('this', 268)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import codecs\n",
    "import os.path\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens(fileName):\n",
    "    #Check first if the file exists\n",
    "    if(os.path.isfile(fileName)):\n",
    "        tf = codecs.open(fileName, mode='r')\n",
    "        text = tf.read()\n",
    "        lowers = text.lower()        \n",
    "        #remove punctuation:\n",
    "        no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "        return tokens\n",
    "\n",
    "print (\"This program will count all words and show you only the n most common words.\")\n",
    "fileName = raw_input(\"Which ascii file should be processed?  \")\n",
    "\n",
    "if(os.path.isfile(fileName)):\n",
    "    n = int(raw_input(\"Up to which rank n should the most common words be displayed?  \"))\n",
    "    tokens = get_tokens(fileName)\n",
    "    count = Counter(tokens)\n",
    "    print count.most_common(n)\n",
    "else:\n",
    "    print(\"Error: This file does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the most common words are articles and prepositions. These very common words which do not convey any inherent information are called *stop words* in natural language processing.  I will remove them in the next processing step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.4: Stop Words\n",
    "Remove stop words, which are common words that do not contain any information but are only gramatical artifacts.\n",
    "Stop words are articles, prepositions, conjunctions (words that connect phrases in a sentence), and connectors (words or phrases that connect independent sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program will count all words and show you only the n most common words.\n",
      "Which ascii file should be processed?  FirstContactWithTensorFlow.txt\n",
      "Up to which rank n should the most common words be displayed?  10\n",
      "[('tensorflow', 193), ('data', 102), ('tensor', 99), ('code', 90), ('learning', 81), ('function', 74), ('one', 73), ('use', 65), ('example', 64), ('available', 63)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import codecs\n",
    "import os.path\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_tokens(fileName):\n",
    "    #Check first if the file exists\n",
    "    if(os.path.isfile(fileName)):\n",
    "        tf = codecs.open(fileName, mode='r')\n",
    "        text = tf.read()\n",
    "        lowers = text.lower()        \n",
    "        #remove punctuation:\n",
    "        no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "        return tokens\n",
    "\n",
    "print (\"This program will count all words and show you only the n most common words.\")\n",
    "fileName = raw_input(\"Which ascii file should be processed?  \")\n",
    "\n",
    "if(os.path.isfile(fileName)):\n",
    "    n = int(raw_input(\"Up to which rank n should the most common words be displayed?  \"))\n",
    "    tokens = get_tokens(fileName)\n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    count = Counter(filtered)\n",
    "    print count.most_common(n)\n",
    "else:\n",
    "    print(\"Error: This file does not exist!\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions:\n",
    "Now the result is as intended. Tensorflow is the most common word in the document. The tokens could now be fed into a model which processes the information. E.g. we could compare the similarity between this document and other documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
